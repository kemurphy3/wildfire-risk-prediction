{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation - NEON AOP Crosswalk\n\n",
    "This notebook provides comprehensive evaluation of the trained crosswalk models, including:\n",
    "- Model performance assessment across different metrics\n",
    "- Spatial and temporal validation\n",
    "- Error analysis and diagnostics\n",
    "- Fire-specific validation results\n",
    "- Integration testing with real satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Geospatial\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium import plugins\n",
    "\n",
    "# Scientific computing\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from scipy import stats\n",
    "\n",
    "# Import our custom modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.features.aop_crosswalk import validate_crosswalk\n",
    "from src.integration.aop_integration import AOPIntegrationManager\n",
    "\n",
    "# Configure settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up directories\n",
    "DATA_DIR = Path('../data')\n",
    "MODELS_DIR = DATA_DIR / 'models'\n",
    "RESULTS_DIR = Path('../results/model_evaluation')\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained Models and Data\n\n",
    "First, we'll load the models trained in the previous notebook along with test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recent models\n",
    "model_files = sorted(MODELS_DIR.glob('*_crosswalk_models_*.pkl'))\n",
    "\n",
    "if len(model_files) >= 2:\n",
    "    # Load linear models\n",
    "    linear_model_path = [f for f in model_files if 'linear' in f.name][-1]\n",
    "    linear_models = joblib.load(linear_model_path)\n",
    "    print(f\"✅ Loaded linear models from: {linear_model_path.name}\")\n",
    "    \n",
    "    # Load ensemble models\n",
    "    ensemble_model_path = [f for f in model_files if 'ensemble' in f.name][-1]\n",
    "    ensemble_models = joblib.load(ensemble_model_path)\n",
    "    print(f\"✅ Loaded ensemble models from: {ensemble_model_path.name}\")\n",
    "else:\n",
    "    print(\"⚠️ No trained models found. Using synthetic models for demonstration.\")\n",
    "    # Create synthetic models for demonstration\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    \n",
    "    target_variables = ['NDVI_AOP', 'NBR_AOP', 'Canopy_Height', 'LAI', 'Biomass']\n",
    "    linear_models = {}\n",
    "    ensemble_models = {}\n",
    "    \n",
    "    for target in target_variables:\n",
    "        linear_models[target] = {\n",
    "            'model': Ridge(alpha=1.0),\n",
    "            'metrics': {'test_r2': 0.75, 'test_mae': 0.1}\n",
    "        }\n",
    "        ensemble_models[target] = {\n",
    "            'model': GradientBoostingRegressor(n_estimators=100),\n",
    "            'metrics': {'test_r2': 0.85, 'test_mae': 0.08}\n",
    "        }\n",
    "\n",
    "# Load metadata\n",
    "metadata_files = sorted(MODELS_DIR.glob('model_metadata_*.json'))\n",
    "if metadata_files:\n",
    "    with open(metadata_files[-1], 'r') as f:\n",
    "        model_metadata = json.load(f)\n",
    "    print(f\"✅ Loaded model metadata from: {metadata_files[-1].name}\")\n",
    "else:\n",
    "    # Create synthetic metadata\n",
    "    model_metadata = {\n",
    "        'target_variables': target_variables,\n",
    "        'satellite_features': ['NDVI', 'NBR', 'NDWI', 'EVI', 'SAVI'],\n",
    "        'sites': {\n",
    "            'fire_sites': ['GRSM', 'SOAP', 'SYCA'],\n",
    "            'baseline_sites': ['SRER', 'JORN', 'ONAQ', 'SJER']\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(f\"\\nTarget variables: {model_metadata['target_variables']}\")\n",
    "print(f\"Satellite features: {model_metadata['satellite_features']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Metrics Assessment\n\n",
    "We'll calculate comprehensive performance metrics for both model types across all target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic test data for evaluation\n",
    "np.random.seed(42)\n",
    "n_test_samples = 200\n",
    "\n",
    "# Satellite features (predictors)\n",
    "X_test = pd.DataFrame({\n",
    "    'NDVI': np.random.normal(0.7, 0.15, n_test_samples),\n",
    "    'NBR': np.random.normal(0.6, 0.2, n_test_samples),\n",
    "    'NDWI': np.random.normal(0.2, 0.1, n_test_samples),\n",
    "    'EVI': np.random.normal(0.5, 0.15, n_test_samples),\n",
    "    'SAVI': np.random.normal(0.4, 0.12, n_test_samples)\n",
    "})\n",
    "\n",
    "# AOP features (ground truth)\n",
    "y_test = pd.DataFrame({\n",
    "    'NDVI_AOP': X_test['NDVI'] * 1.1 + np.random.normal(0, 0.05, n_test_samples),\n",
    "    'NBR_AOP': X_test['NBR'] * 1.15 + np.random.normal(0, 0.06, n_test_samples),\n",
    "    'Canopy_Height': 15 + 20 * X_test['NDVI'] + np.random.normal(0, 2, n_test_samples),\n",
    "    'LAI': 2 + 3 * X_test['NDVI'] + np.random.normal(0, 0.3, n_test_samples),\n",
    "    'Biomass': 50 + 100 * X_test['NDVI'] + np.random.normal(0, 10, n_test_samples)\n",
    "})\n",
    "\n",
    "# Clip to realistic ranges\n",
    "y_test['NDVI_AOP'] = np.clip(y_test['NDVI_AOP'], -1, 1)\n",
    "y_test['NBR_AOP'] = np.clip(y_test['NBR_AOP'], -1, 1)\n",
    "y_test['Canopy_Height'] = np.clip(y_test['Canopy_Height'], 0, 50)\n",
    "y_test['LAI'] = np.clip(y_test['LAI'], 0, 8)\n",
    "y_test['Biomass'] = np.clip(y_test['Biomass'], 0, 300)\n",
    "\n",
    "print(f\"Test dataset shape: {X_test.shape}\")\n",
    "print(f\"Target variables shape: {y_test.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}