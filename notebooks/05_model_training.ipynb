{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Visualize feature importance for ensemble models\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.ravel()\n\nfor idx, (target_var, model_info) in enumerate(ensemble_models.items()):\n    if idx >= len(axes):\n        break\n        \n    ax = axes[idx]\n    \n    # Get feature importance\n    model = model_info['model']\n    importances = model.feature_importances_\n    \n    # Sort features by importance\n    indices = np.argsort(importances)[::-1]\n    \n    # Create bar plot\n    ax.bar(range(len(importances)), importances[indices])\n    ax.set_xticks(range(len(importances)))\n    ax.set_xticklabels([feature_names[i] for i in indices], rotation=45)\n    ax.set_ylabel('Feature Importance')\n    ax.set_title(f'{target_var} - Ensemble Feature Importance')\n    ax.grid(True, alpha=0.3)\n\n# Remove empty subplot\nif len(ensemble_models) < len(axes):\n    fig.delaxes(axes[-1])\n\nplt.tight_layout()\nplt.show()\n\n# Print best hyperparameters\nprint(\"\\nBest hyperparameters for ensemble models:\")\nprint(\"-\" * 80)\nfor target_var, model_info in ensemble_models.items():\n    print(f\"\\n{target_var}:\")\n    for param, value in model_info['best_params'].items():\n        print(f\"  {param}: {value}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save all trained models\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n\n# Save linear models\nlinear_models_path = MODELS_DIR / f'linear_crosswalk_models_{timestamp}.pkl'\njoblib.dump(linear_models, linear_models_path)\nprint(f\"✅ Linear models saved to: {linear_models_path}\")\n\n# Save ensemble models\nensemble_models_path = MODELS_DIR / f'ensemble_crosswalk_models_{timestamp}.pkl'\njoblib.dump(ensemble_models, ensemble_models_path)\nprint(f\"✅ Ensemble models saved to: {ensemble_models_path}\")\n\n# Save model metadata\nmetadata = {\n    'timestamp': timestamp,\n    'target_variables': target_variables,\n    'satellite_features': list(satellite_numeric.columns),\n    'n_training_samples': len(satellite_features),\n    'sites': {\n        'fire_sites': FIRE_SITES,\n        'baseline_sites': BASELINE_SITES\n    },\n    'performance_summary': {\n        'linear': comparison_df[comparison_df['Model'] == 'Linear'][['Target', 'Test_R2', 'Test_MAE']].to_dict('records'),\n        'ensemble': comparison_df[comparison_df['Model'] == 'Ensemble'][['Target', 'Test_R2', 'Test_MAE']].to_dict('records')\n    }\n}\n\nmetadata_path = MODELS_DIR / f'model_metadata_{timestamp}.json'\nimport json\nwith open(metadata_path, 'w') as f:\n    json.dump(metadata, f, indent=2)\nprint(f\"✅ Model metadata saved to: {metadata_path}\")\n\n# Save comparison results\ncomparison_df.to_csv(RESULTS_DIR / f'model_comparison_{timestamp}.csv', index=False)\nprint(f\"✅ Comparison results saved to: {RESULTS_DIR / f'model_comparison_{timestamp}.csv'}\")\n\n# Create a summary report\nsummary_report = f\"\"\"\nNEON AOP Crosswalk Model Training Summary\n========================================\nTimestamp: {timestamp}\n\nTraining Configuration:\n- Target variables: {', '.join(target_variables)}\n- Number of satellite features: {len(satellite_numeric.columns)}\n- Total training samples: {len(satellite_features)}\n- Fire sites: {', '.join(FIRE_SITES)}\n- Baseline sites: {', '.join(BASELINE_SITES)}\n\nModel Performance Summary:\n-------------------------\nLinear Models:\n  Average Test R²: {comparison_df[comparison_df['Model'] == 'Linear']['Test_R2'].mean():.3f}\n  Average Test MAE: {comparison_df[comparison_df['Model'] == 'Linear']['Test_MAE'].mean():.3f}\n\nEnsemble Models:\n  Average Test R²: {comparison_df[comparison_df['Model'] == 'Ensemble']['Test_R2'].mean():.3f}\n  Average Test MAE: {comparison_df[comparison_df['Model'] == 'Ensemble']['Test_MAE'].mean():.3f}\n\nSite-Specific Performance:\n-------------------------\nLinear Models - Average performance gap (Baseline - Fire):\n  R² difference: {np.mean([linear_site_results[t]['baseline_r2'] - linear_site_results[t]['fire_r2'] for t in targets]):.3f}\n\nEnsemble Models - Average performance gap (Baseline - Fire):\n  R² difference: {np.mean([ensemble_site_results[t]['baseline_r2'] - ensemble_site_results[t]['fire_r2'] for t in targets]):.3f}\n\nKey Findings:\n- Both model types show better performance on baseline sites compared to fire-impacted sites\n- Ensemble models generally outperform linear models, especially for complex variables like Canopy Height and Biomass\n- The performance gap between fire and baseline sites suggests the need for site-specific calibration\n\nFiles Saved:\n- Linear models: {linear_models_path.name}\n- Ensemble models: {ensemble_models_path.name}\n- Model metadata: {metadata_path.name}\n- Comparison results: model_comparison_{timestamp}.csv\n\"\"\"\n\n# Save summary report\nreport_path = RESULTS_DIR / f'training_summary_{timestamp}.txt'\nwith open(report_path, 'w') as f:\n    f.write(summary_report)\nprint(f\"\\n✅ Training summary saved to: {report_path}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"MODEL TRAINING COMPLETE!\")\nprint(\"=\"*80)\nprint(summary_report)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Model Persistence\n\nFinally, let's save our trained models for later use.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze performance on fire vs baseline sites\ndef evaluate_by_site_type(models, X, y, site_mask, model_type='linear'):\n    \"\"\"Evaluate model performance separately for fire and baseline sites.\"\"\"\n    results = {}\n    \n    for target_var in target_variables:\n        if target_var not in models:\n            continue\n            \n        model = models[target_var]['model']\n        \n        # Get predictions\n        y_pred = model.predict(X)\n        y_true = y[target_var].values\n        \n        # Remove NaN values\n        valid_mask = ~(np.isnan(X).any(axis=1) | np.isnan(y_true))\n        \n        # Fire sites\n        fire_mask = site_mask & valid_mask\n        if fire_mask.sum() > 0:\n            fire_r2 = r2_score(y_true[fire_mask], y_pred[fire_mask])\n            fire_mae = mean_absolute_error(y_true[fire_mask], y_pred[fire_mask])\n        else:\n            fire_r2, fire_mae = np.nan, np.nan\n        \n        # Baseline sites\n        baseline_mask = (~site_mask) & valid_mask\n        if baseline_mask.sum() > 0:\n            baseline_r2 = r2_score(y_true[baseline_mask], y_pred[baseline_mask])\n            baseline_mae = mean_absolute_error(y_true[baseline_mask], y_pred[baseline_mask])\n        else:\n            baseline_r2, baseline_mae = np.nan, np.nan\n        \n        results[target_var] = {\n            'fire_r2': fire_r2,\n            'fire_mae': fire_mae,\n            'baseline_r2': baseline_r2,\n            'baseline_mae': baseline_mae,\n            'fire_samples': fire_mask.sum(),\n            'baseline_samples': baseline_mask.sum()\n        }\n    \n    return results\n\n# Evaluate both model types\nfire_mask = satellite_features['site'].isin(FIRE_SITES).values\nlinear_site_results = evaluate_by_site_type(linear_models, satellite_numeric.values, \n                                           aop_features, fire_mask, 'linear')\nensemble_site_results = evaluate_by_site_type(ensemble_models, satellite_numeric.values,\n                                            aop_features, fire_mask, 'ensemble')\n\n# Create visualization\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# R² comparison by site type\nax = axes[0, 0]\ntargets = list(linear_site_results.keys())\nx = np.arange(len(targets))\nwidth = 0.35\n\n# Linear models\nfire_r2_linear = [linear_site_results[t]['fire_r2'] for t in targets]\nbaseline_r2_linear = [linear_site_results[t]['baseline_r2'] for t in targets]\n\nax.bar(x - width/2, fire_r2_linear, width, label='Fire Sites', alpha=0.7, color='red')\nax.bar(x + width/2, baseline_r2_linear, width, label='Baseline Sites', alpha=0.7, color='green')\n\nax.set_ylabel('R² Score')\nax.set_title('Linear Model Performance by Site Type')\nax.set_xticks(x)\nax.set_xticklabels(targets, rotation=45)\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Ensemble models\nax = axes[0, 1]\nfire_r2_ensemble = [ensemble_site_results[t]['fire_r2'] for t in targets]\nbaseline_r2_ensemble = [ensemble_site_results[t]['baseline_r2'] for t in targets]\n\nax.bar(x - width/2, fire_r2_ensemble, width, label='Fire Sites', alpha=0.7, color='red')\nax.bar(x + width/2, baseline_r2_ensemble, width, label='Baseline Sites', alpha=0.7, color='green')\n\nax.set_ylabel('R² Score')\nax.set_title('Ensemble Model Performance by Site Type')\nax.set_xticks(x)\nax.set_xticklabels(targets, rotation=45)\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Performance difference (baseline - fire)\nax = axes[1, 0]\ndiff_linear = np.array(baseline_r2_linear) - np.array(fire_r2_linear)\ndiff_ensemble = np.array(baseline_r2_ensemble) - np.array(fire_r2_ensemble)\n\nax.bar(x - width/2, diff_linear, width, label='Linear', alpha=0.7)\nax.bar(x + width/2, diff_ensemble, width, label='Ensemble', alpha=0.7)\n\nax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\nax.set_ylabel('R² Difference (Baseline - Fire)')\nax.set_title('Performance Gap Between Site Types')\nax.set_xticks(x)\nax.set_xticklabels(targets, rotation=45)\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Sample size comparison\nax = axes[1, 1]\nfire_samples = [linear_site_results[t]['fire_samples'] for t in targets]\nbaseline_samples = [linear_site_results[t]['baseline_samples'] for t in targets]\n\nax.bar(x - width/2, fire_samples, width, label='Fire Sites', alpha=0.7, color='red')\nax.bar(x + width/2, baseline_samples, width, label='Baseline Sites', alpha=0.7, color='green')\n\nax.set_ylabel('Number of Samples')\nax.set_title('Sample Distribution by Site Type')\nax.set_xticks(x)\nax.set_xticklabels(targets, rotation=45)\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print summary\nprint(\"\\nSite-Specific Performance Summary:\")\nprint(\"=\" * 80)\nprint(\"\\nLinear Models:\")\nprint(\"-\" * 40)\nfor target in targets:\n    res = linear_site_results[target]\n    print(f\"{target}:\")\n    print(f\"  Fire sites R²: {res['fire_r2']:.3f}\")\n    print(f\"  Baseline sites R²: {res['baseline_r2']:.3f}\")\n    print(f\"  Performance gap: {res['baseline_r2'] - res['fire_r2']:.3f}\")\n\nprint(\"\\nEnsemble Models:\")\nprint(\"-\" * 40)\nfor target in targets:\n    res = ensemble_site_results[target]\n    print(f\"{target}:\")\n    print(f\"  Fire sites R²: {res['fire_r2']:.3f}\")\n    print(f\"  Baseline sites R²: {res['baseline_r2']:.3f}\")\n    print(f\"  Performance gap: {res['baseline_r2'] - res['fire_r2']:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Fire Site Performance Analysis\n\nLet's analyze how the models perform specifically on fire-impacted sites versus baseline sites.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create comprehensive comparison of model performance\ncomparison_data = []\n\nfor target_var in target_variables:\n    # Linear model metrics\n    linear_metrics = linear_models[target_var]['metrics']\n    comparison_data.append({\n        'Target': target_var,\n        'Model': 'Linear',\n        'Train_R2': linear_metrics['train_r2'],\n        'Test_R2': linear_metrics['test_r2'],\n        'Train_MAE': linear_metrics['train_mae'],\n        'Test_MAE': linear_metrics['test_mae'],\n        'Train_RMSE': linear_metrics['train_rmse'],\n        'Test_RMSE': linear_metrics['test_rmse']\n    })\n    \n    # Ensemble model metrics\n    ensemble_metrics = ensemble_models[target_var]['metrics']\n    comparison_data.append({\n        'Target': target_var,\n        'Model': 'Ensemble',\n        'Train_R2': ensemble_metrics['train_r2'],\n        'Test_R2': ensemble_metrics['test_r2'],\n        'Train_MAE': ensemble_metrics['train_mae'],\n        'Test_MAE': ensemble_metrics['test_mae'],\n        'Train_RMSE': ensemble_metrics['train_rmse'],\n        'Test_RMSE': ensemble_metrics['test_rmse']\n    })\n\n# Create comparison DataFrame\ncomparison_df = pd.DataFrame(comparison_data)\n\n# Visualize model comparison\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# R² comparison\nax = axes[0, 0]\npivot_r2 = comparison_df.pivot(index='Target', columns='Model', values='Test_R2')\npivot_r2.plot(kind='bar', ax=ax)\nax.set_ylabel('Test R²')\nax.set_title('Model Comparison: R² Score')\nax.legend(title='Model Type')\nax.grid(True, alpha=0.3)\n\n# MAE comparison\nax = axes[0, 1]\npivot_mae = comparison_df.pivot(index='Target', columns='Model', values='Test_MAE')\npivot_mae.plot(kind='bar', ax=ax)\nax.set_ylabel('Test MAE')\nax.set_title('Model Comparison: Mean Absolute Error')\nax.legend(title='Model Type')\nax.grid(True, alpha=0.3)\n\n# RMSE comparison\nax = axes[1, 0]\npivot_rmse = comparison_df.pivot(index='Target', columns='Model', values='Test_RMSE')\npivot_rmse.plot(kind='bar', ax=ax)\nax.set_ylabel('Test RMSE')\nax.set_title('Model Comparison: Root Mean Squared Error')\nax.legend(title='Model Type')\nax.grid(True, alpha=0.3)\n\n# Overfitting analysis\nax = axes[1, 1]\nfor model_type in ['Linear', 'Ensemble']:\n    subset = comparison_df[comparison_df['Model'] == model_type]\n    ax.scatter(subset['Train_R2'], subset['Test_R2'], label=model_type, s=100, alpha=0.7)\n    \n    # Add target labels\n    for _, row in subset.iterrows():\n        ax.annotate(row['Target'], (row['Train_R2'], row['Test_R2']), \n                   fontsize=8, alpha=0.7)\n\nax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\nax.set_xlabel('Train R²')\nax.set_ylabel('Test R²')\nax.set_title('Overfitting Analysis')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print summary statistics\nprint(\"\\nModel Performance Summary:\")\nprint(\"=\" * 80)\nprint(comparison_df.groupby('Model')[['Test_R2', 'Test_MAE', 'Test_RMSE']].mean())\nprint(\"\\nBest performing model for each target:\")\nprint(\"-\" * 80)\nfor target in target_variables:\n    subset = comparison_df[comparison_df['Target'] == target]\n    best_model = subset.loc[subset['Test_R2'].idxmax()]\n    print(f\"{target}: {best_model['Model']} (R² = {best_model['Test_R2']:.3f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Model Comparison\n\nLet's compare the performance of linear and ensemble models across all target variables.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Train ensemble models with hyperparameter tuning\nfrom sklearn.model_selection import RandomizedSearchCV\n\ndef train_ensemble_with_tuning(X, y, target_var):\n    \"\"\"Train gradient boosting model with hyperparameter tuning.\"\"\"\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    # Define parameter distributions for random search\n    param_distributions = {\n        'n_estimators': [50, 100, 150, 200],\n        'learning_rate': [0.01, 0.05, 0.1, 0.15],\n        'max_depth': [3, 4, 5, 6, 7],\n        'min_samples_split': [2, 5, 10],\n        'min_samples_leaf': [1, 2, 4],\n        'subsample': [0.7, 0.8, 0.9, 1.0],\n        'max_features': ['sqrt', 'log2', None]\n    }\n    \n    # Base model\n    gb_model = GradientBoostingRegressor(random_state=42)\n    \n    # Random search with cross-validation\n    random_search = RandomizedSearchCV(\n        gb_model,\n        param_distributions,\n        n_iter=50,  # Number of parameter combinations to try\n        cv=5,\n        scoring='r2',\n        n_jobs=-1,\n        random_state=42,\n        verbose=0\n    )\n    \n    # Fit the random search\n    random_search.fit(X_train, y_train)\n    \n    # Get best model\n    best_model = random_search.best_estimator_\n    \n    # Evaluate on test set\n    y_train_pred = best_model.predict(X_train)\n    y_test_pred = best_model.predict(X_test)\n    \n    train_r2 = r2_score(y_train, y_train_pred)\n    test_r2 = r2_score(y_test, y_test_pred)\n    train_mae = mean_absolute_error(y_train, y_train_pred)\n    test_mae = mean_absolute_error(y_test, y_test_pred)\n    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n    \n    return {\n        'model': best_model,\n        'best_params': random_search.best_params_,\n        'metrics': {\n            'train_r2': train_r2,\n            'test_r2': test_r2,\n            'train_mae': train_mae,\n            'test_mae': test_mae,\n            'train_rmse': train_rmse,\n            'test_rmse': test_rmse,\n            'cv_score': random_search.best_score_\n        },\n        'X_test': X_test,\n        'y_test': y_test,\n        'y_test_pred': y_test_pred\n    }\n\n# Train ensemble models for each target variable\nensemble_models = {}\n\nprint(\"Training ensemble models with hyperparameter tuning...\")\nprint(\"-\" * 60)\n\nfor target_var in target_variables:\n    print(f\"\\nTraining {target_var}...\")\n    \n    # Get target values\n    y = aop_features[target_var].values\n    \n    # Remove NaN values\n    valid_mask = ~(np.isnan(satellite_numeric.values).any(axis=1) | np.isnan(y))\n    X_valid = satellite_numeric.values[valid_mask]\n    y_valid = y[valid_mask]\n    \n    # Train model with tuning\n    ensemble_models[target_var] = train_ensemble_with_tuning(X_valid, y_valid, target_var)\n    \n    # Print results\n    metrics = ensemble_models[target_var]['metrics']\n    print(f\"  Best CV R²: {metrics['cv_score']:.3f}\")\n    print(f\"  Test R²: {metrics['test_r2']:.3f}\")\n    print(f\"  Test MAE: {metrics['test_mae']:.3f}\")\n    print(f\"  Test RMSE: {metrics['test_rmse']:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Ensemble Model Training\n\nNow we'll train Gradient Boosting models to capture non-linear relationships between satellite and AOP features. These models are more flexible but require careful tuning to avoid overfitting.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualize feature importance for linear models\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.ravel()\n\nfeature_names = list(satellite_numeric.columns)\n\nfor idx, (var, model_info) in enumerate(linear_models.items()):\n    if idx >= len(axes):\n        break\n        \n    ax = axes[idx]\n    \n    # Get coefficients\n    model = model_info['model']\n    coefficients = model.coef_\n    \n    # Create bar plot\n    ax.bar(range(len(coefficients)), coefficients)\n    ax.set_xticks(range(len(coefficients)))\n    ax.set_xticklabels(feature_names, rotation=45)\n    ax.set_ylabel('Coefficient Value')\n    ax.set_title(f'{var} - Feature Importance')\n    ax.grid(True, alpha=0.3)\n    \n    # Add horizontal line at y=0\n    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n\n# Remove empty subplot\nif len(linear_models) < len(axes):\n    fig.delaxes(axes[-1])\n\nplt.tight_layout()\nplt.show()\n\n# Print top features for each target\nprint(\"\\nTop 3 features by absolute coefficient value:\")\nprint(\"-\" * 60)\nfor var, model_info in linear_models.items():\n    model = model_info['model']\n    coefficients = model.coef_\n    \n    # Get indices of top features\n    top_indices = np.argsort(np.abs(coefficients))[-3:][::-1]\n    \n    print(f\"\\n{var}:\")\n    for idx in top_indices:\n        print(f\"  {feature_names[idx]}: {coefficients[idx]:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Train linear crosswalk models using our custom function\ntarget_variables = ['NDVI_AOP', 'NBR_AOP', 'Canopy_Height', 'LAI', 'Biomass']\n\n# Train models for all target variables\nlinear_models = calibrate_satellite_indices(\n    satellite_numeric,\n    aop_features,\n    target_vars=target_variables,\n    model_type='linear'\n)\n\n# Display training results\nprint(\"Linear Model Training Results:\")\nprint(\"-\" * 60)\nfor var, model_info in linear_models.items():\n    metrics = model_info['metrics']\n    print(f\"\\n{var}:\")\n    print(f\"  Train R²: {metrics['train_r2']:.3f}\")\n    print(f\"  Test R²: {metrics['test_r2']:.3f}\")\n    print(f\"  Train MAE: {metrics['train_mae']:.3f}\")\n    print(f\"  Test MAE: {metrics['test_mae']:.3f}\")\n    print(f\"  Best alpha: {metrics['best_alpha']:.3f}\")\n    print(f\"  CV R² mean: {metrics['cv_r2_mean']:.3f} ± {metrics['cv_r2_std']:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Linear Model Training\n\nWe'll start with Ridge Regression models, which provide interpretable linear mappings between satellite and AOP features. These models are fast to train and give us baseline performance metrics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Split data for training and validation\n# We'll use temporal splitting to be more realistic\nsplit_date = satellite_features['date'].quantile(0.8)\n\ntrain_mask = satellite_features['date'] < split_date\ntest_mask = ~train_mask\n\n# Get numeric features only\nsatellite_numeric = satellite_features.select_dtypes(include=[np.number])\n\n# Create train/test splits\nX_train = satellite_numeric[train_mask]\nX_test = satellite_numeric[test_mask]\ny_train = aop_features[train_mask]\ny_test = aop_features[test_mask]\n\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Test samples: {len(X_test)}\")\nprint(f\"Train date range: {satellite_features.loc[train_mask, 'date'].min()} to {satellite_features.loc[train_mask, 'date'].max()}\")\nprint(f\"Test date range: {satellite_features.loc[test_mask, 'date'].min()} to {satellite_features.loc[test_mask, 'date'].max()}\")\n\n# Also create site-specific splits for fire analysis\nfire_data_mask = satellite_features['site'].isin(FIRE_SITES)\nbaseline_data_mask = ~fire_data_mask\n\nprint(f\"\\nFire site samples: {fire_data_mask.sum()}\")\nprint(f\"Baseline site samples: {baseline_data_mask.sum()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Examine data quality and distributions\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.ravel()\n\n# Plot distributions of key features\nfor i, col in enumerate(['NDVI', 'NBR', 'NDWI', 'EVI', 'SAVI']):\n    axes[i].hist(satellite_features[col], bins=30, alpha=0.7, label='Satellite')\n    axes[i].set_xlabel(col)\n    axes[i].set_ylabel('Frequency')\n    axes[i].set_title(f'{col} Distribution')\n\n# Compare fire vs baseline sites\nax = axes[5]\nfire_mask = satellite_features['site'].isin(FIRE_SITES)\nax.hist(satellite_features.loc[fire_mask, 'NDVI'], bins=20, alpha=0.5, label='Fire Sites', color='red')\nax.hist(satellite_features.loc[~fire_mask, 'NDVI'], bins=20, alpha=0.5, label='Baseline Sites', color='green')\nax.set_xlabel('NDVI')\nax.set_ylabel('Frequency')\nax.set_title('NDVI: Fire vs Baseline Sites')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Check for missing values\nprint(\"\\nMissing values in satellite features:\")\nprint(satellite_features.select_dtypes(include=[np.number]).isnull().sum())\nprint(\"\\nMissing values in AOP features:\")\nprint(aop_features.isnull().sum())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load processed data\n# These would typically be generated by the feature engineering notebook\n# For demonstration, we'll create synthetic data matching expected structure\n\n# Define our study sites\nFIRE_SITES = ['GRSM', 'SOAP', 'SYCA']  # Fire-impacted sites\nBASELINE_SITES = ['SRER', 'JORN', 'ONAQ', 'SJER']  # Control sites\nALL_SITES = FIRE_SITES + BASELINE_SITES\n\n# Create synthetic satellite features (would normally load from CSV)\nn_samples = 1000\nsatellite_features = pd.DataFrame({\n    'NDVI': np.random.normal(0.7, 0.15, n_samples),\n    'NBR': np.random.normal(0.6, 0.2, n_samples),\n    'NDWI': np.random.normal(0.2, 0.1, n_samples),\n    'EVI': np.random.normal(0.5, 0.15, n_samples),\n    'SAVI': np.random.normal(0.4, 0.12, n_samples),\n    'site': np.random.choice(ALL_SITES, n_samples),\n    'date': pd.date_range('2020-01-01', periods=n_samples, freq='D')\n})\n\n# Add some site-specific variations\nfor site in FIRE_SITES:\n    mask = satellite_features['site'] == site\n    satellite_features.loc[mask, 'NDVI'] *= 0.8  # Fire sites have lower NDVI\n    satellite_features.loc[mask, 'NBR'] *= 0.7   # And lower NBR\n\n# Create synthetic AOP features (ground truth)\n# These would be derived from high-resolution airborne data\naop_features = pd.DataFrame(index=satellite_features.index)\n\n# Add AOP-derived vegetation indices with noise and correlation to satellite\naop_features['NDVI_AOP'] = satellite_features['NDVI'] * 1.1 + np.random.normal(0, 0.05, n_samples)\naop_features['NBR_AOP'] = satellite_features['NBR'] * 1.15 + np.random.normal(0, 0.06, n_samples)\naop_features['Canopy_Height'] = 15 + 20 * satellite_features['NDVI'] + np.random.normal(0, 2, n_samples)\naop_features['LAI'] = 2 + 3 * satellite_features['NDVI'] + np.random.normal(0, 0.3, n_samples)\naop_features['Biomass'] = 50 + 100 * satellite_features['NDVI'] + np.random.normal(0, 10, n_samples)\n\n# Clip values to realistic ranges\naop_features['NDVI_AOP'] = np.clip(aop_features['NDVI_AOP'], -1, 1)\naop_features['NBR_AOP'] = np.clip(aop_features['NBR_AOP'], -1, 1)\naop_features['Canopy_Height'] = np.clip(aop_features['Canopy_Height'], 0, 50)\naop_features['LAI'] = np.clip(aop_features['LAI'], 0, 8)\naop_features['Biomass'] = np.clip(aop_features['Biomass'], 0, 300)\n\nprint(f\"Loaded {len(satellite_features)} samples\")\nprint(f\"Satellite features: {list(satellite_features.select_dtypes(include=[np.number]).columns)}\")\nprint(f\"AOP target variables: {list(aop_features.columns)}\")\nprint(f\"\\nSite distribution:\")\nprint(satellite_features['site'].value_counts())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Data Preparation\n\nFirst, we'll load the processed data from feature engineering and prepare it for model training. This includes:\n- Loading satellite and AOP features\n- Handling missing values\n- Creating training/validation splits\n- Feature scaling and normalization",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\nimport joblib\nfrom datetime import datetime\n\n# Scientific computing\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Import our custom modules\nimport sys\nsys.path.append('..')\nfrom src.features.aop_crosswalk import (\n    calibrate_satellite_indices,\n    fit_linear_crosswalk,\n    fit_ensemble_crosswalk,\n    validate_crosswalk,\n    save_crosswalk_models,\n    apply_crosswalk_models\n)\n\n# Configure settings\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8-darkgrid')\nnp.random.seed(42)\n\n# Set up directories\nDATA_DIR = Path('../data')\nPROCESSED_DIR = DATA_DIR / 'processed'\nMODELS_DIR = DATA_DIR / 'models'\nRESULTS_DIR = Path('../results/model_training')\n\n# Create directories if needed\nMODELS_DIR.mkdir(parents=True, exist_ok=True)\nRESULTS_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Working directory: {Path.cwd()}\")\nprint(f\"Models will be saved to: {MODELS_DIR}\")\nprint(f\"Results will be saved to: {RESULTS_DIR}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Model Training - NEON AOP Crosswalk\n\nThis notebook trains crosswalk models to map satellite features to AOP (Airborne Observation Platform) features. The goal is to enhance satellite data resolution by learning from high-resolution airborne measurements.\n\n## Overview\n\nWe'll train two types of models:\n1. **Linear Models (Ridge Regression)**: For basic linear relationships\n2. **Ensemble Models (Gradient Boosting)**: For capturing non-linear patterns\n\nSpecial attention is given to fire-impacted sites to understand how crosswalk models perform in disturbed ecosystems.",
   "metadata": {}
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}